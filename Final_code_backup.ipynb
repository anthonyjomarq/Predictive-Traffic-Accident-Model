{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Predictive Traffic Accident Model\n\n## Analysis of 2021 NHTSA FARS Data for Fatality Prediction and Pattern Recognition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Project Overview\n\n**Objective**: This project analyzes traffic accident data from 2021 to predict fatalities and identify patterns in accident occurrences using machine learning techniques.\n\n**Goals**:\n- Predict the number of fatalities in traffic accidents using supervised learning\n- Identify clusters and patterns in accident data using unsupervised learning  \n- Extract actionable insights about contributing factors to traffic accidents\n\n### Dataset Description\n\nThe dataset consists of comprehensive traffic accident records from the **Fatality Analysis Reporting System (FARS)** for 2021, maintained by the National Highway Traffic Safety Administration (NHTSA). \n\n**Data Sources**:\n- **ACC_AUX.CSV**: Accident-level data including location, time, weather, and crash characteristics\n- **VEH_AUX.CSV**: Vehicle-level data including vehicle type, model year, and driver information\n- **PER_AUX.CSV**: Person-level data including age, injury severity, and demographic information\n\nThe merged dataset contains detailed information about each accident including:\n- Case numbers and geographical information (state, county)\n- Temporal factors (time of day, day of week)\n- Vehicle characteristics (type, model year)\n- Person demographics and injury details\n- Environmental conditions and contributing factors"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Environment Setup and Dependencies\n\n### Python Libraries Used\n\n**Data Manipulation & Analysis**:\n- **Pandas**: Primary tool for data manipulation, merging datasets, and exploratory data analysis\n- **NumPy**: Numerical computations and array operations\n\n**Machine Learning**:\n- **Scikit-learn**: Comprehensive machine learning library providing:\n  - Linear Regression for supervised learning (fatality prediction)\n  - K-Means clustering for unsupervised pattern discovery\n  - Cross-validation tools for model evaluation\n  - Preprocessing utilities (StandardScaler, OneHotEncoder)\n\n**Data Visualization**:\n- **Matplotlib**: Core plotting library for creating static visualizations\n- **Seaborn**: Statistical visualization library built on matplotlib for enhanced plotting\n- **Plotly**: Interactive visualization library for dynamic plots and dashboards\n\n**Statistical Analysis**:\n- **SciPy**: Statistical functions and hypothesis testing tools\n\n### Methodology Overview\n\n**Data Integration Approach**: \n- Merge three separate FARS datasets using case numbers as primary keys\n- Handle missing values and data quality issues systematically\n- Create meaningful categorical mappings for encoded variables\n\n**Feature Engineering Strategy**:\n- Transform categorical codes into interpretable labels\n- Create combined features to capture complex relationships\n- Standardize numerical features for machine learning algorithms"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plotly Packages (optional - comment out if not available)\ntry:\n    from plotly import tools\n    import plotly.figure_factory as ff\n    import plotly.graph_objs as go\n    from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n    init_notebook_mode(connected=True)\n    PLOTLY_AVAILABLE = True\nexcept ImportError:\n    print(\"Plotly not available - using matplotlib/seaborn for all visualizations\")\n    PLOTLY_AVAILABLE = False\n\n# Statistical Libraries\nfrom scipy.stats import norm\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nfrom scipy import stats\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Folium for mapping (optional - comment out if not available)\ntry:\n    import folium\n    from folium.plugins import HeatMap\n    FOLIUM_AVAILABLE = True\nexcept ImportError:\n    print(\"Folium not available - skipping map visualizations\")\n    FOLIUM_AVAILABLE = False\n\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Loading and Initial Exploration\n\nThis section loads the three FARS datasets and performs initial data quality assessment:\n\n- **Data Quality Check**: Identify missing values and data inconsistencies\n- **Data Integration**: Merge datasets using case numbers as primary keys\n- **Initial Statistics**: Basic descriptive statistics for understanding data distribution"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# reading the files \n",
    "\n",
    "df_veh_aux = pd.read_csv('../datasets/VEH_AUX.CSV')\n",
    "df_veh_aux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# check if there are missing values \n",
    "\n",
    "missing_values = df_veh_aux.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# drop missing value columns \n",
    "df_veh_aux = df_veh_aux.drop(columns=['A_IMP2'])\n",
    "df_veh_aux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "df_veh_aux.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# reading the file and print the head \n",
    "\n",
    "df_per_aux = pd.read_csv('../datasets/PER_AUX.CSV')\n",
    "df_per_aux.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check for missing values in vehicle dataset\nmissing_values = df_veh_aux.isnull().sum()\nprint(\"Missing values in Vehicle dataset:\")\nprint(missing_values)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "df_per_aux.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "df_acc_aux = pd.read_csv('../datasets/ACC_AUX.CSV')\n",
    "df_acc_aux.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check for missing values in accident dataset\nmissing_values = df_acc_aux.isnull().sum()\nprint(\"Missing values in Accident dataset:\")\nprint(missing_values)"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "df_acc_aux.describe()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Verify case number consistency across datasets\nprint(\"Case number samples from each dataset:\")\nprint(\"Accident dataset ST_CASE:\", df_acc_aux['ST_CASE'].head())\nprint(\"Person dataset ST_CASE:\", df_per_aux['ST_CASE'].head())\nprint(\"Vehicle dataset ST_CASE:\", df_veh_aux['ST_CASE'].head())"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Data Integration: Merge the three datasets\nprint(\"Merging datasets...\")\n\n# Merge vehicle and person data first\nmerged_veh_per = pd.merge(df_veh_aux, df_per_aux, on='ST_CASE', how='inner')\n\n# Then merge with accident data\nmerged_all = pd.merge(df_acc_aux, merged_veh_per, on='ST_CASE', how='inner')\n\n# Check for duplicate rows and columns\nduplicate_rows = merged_all.duplicated().sum()\nprint(\"Number of duplicate rows:\", duplicate_rows)\n\nduplicate_columns = merged_all.columns.duplicated().sum()\nprint(\"Number of duplicate columns:\", duplicate_columns)\n\n# Remove duplicates if present\nif duplicate_rows > 0:\n    merged_all = merged_all.drop_duplicates()\n    print(\"Duplicate rows removed.\")\n\n# Remove duplicate columns\nmerged_all = merged_all.loc[:,~merged_all.columns.duplicated()]\n\nprint(f\"Final merged dataset shape: {merged_all.shape}\")\nprint(\"\\nFirst 5 rows of merged dataset:\")\nprint(merged_all.head())"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# Compare YEAR_x and 'YEAR_y' to check if they are the same\n",
    "same_year = (merged_all['YEAR_x'] == merged_all['YEAR_y']).all()\n",
    "print(f\"YEAR_x and YEAR_y are same {same_year}\")\n",
    "\n",
    "# Compare VEH_NO_x and y  \n",
    "same_veh_no = (merged_all['VEH_NO_x'] == merged_all['VEH_NO_y']).all()\n",
    "print(f\"VEH_NO_x and VEH_NO_y are the same: {same_veh_no}\")\n",
    "\n",
    "# Drop duplicate columns \n",
    "if same_year:\n",
    "    merged_all.drop(columns=['YEAR_y'], inplace=True)\n",
    "if same_veh_no:\n",
    "    merged_all.drop(columns=['VEH_NO_y'], inplace=True)\n",
    "\n",
    "# rename columns \n",
    "if not same_year or not same_veh_no:\n",
    "    merged_all.rename(columns={'YEAR_x': 'Year_Accident', 'YEAR_y': 'Year_Vehicle',\n",
    "                               'VEH_NO_x': 'Vehicle_No_Accident', 'VEH_NO_y': 'Vehicle_No_Person'}, inplace=True)\n",
    "\n",
    "# Check the updated dataframe\n",
    "print(merged_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "merged_all.head()\n",
    "merged_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "merged_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "merged_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Exploratory Data Analysis and Visualization\n\nThis section explores key patterns in the traffic accident data through statistical analysis and visualization."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Key Insights from Exploratory Analysis\n\n**Temporal Patterns**: \n- **Weekday vs. Weekend Distribution**: Analysis reveals distinct patterns in accident frequency between weekdays and weekends, potentially related to commuter traffic patterns and recreational driving behaviors.\n\n**Demographic Analysis**:\n- **Age Group Involvement**: The distribution of crashes across different age groups provides insights into driving experience and risk-taking behaviors. Younger age groups (16-24 years) often show higher involvement rates, suggesting factors like inexperience, risk-taking behavior, or distracted driving.\n\n**Vehicle and Environmental Factors**:\n- **Time of Day Impact**: Accident patterns vary significantly by time of day, with implications for targeted safety interventions and traffic management.\n- **Vehicle Type Correlation**: Different vehicle types show varying involvement rates in fatal accidents, informing vehicle safety feature development."
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n",
    "# check the describe for these columns \n",
    "print(merged_all[['A_TOD', 'A_DOW', 'A_BODY']].describe())\n",
    "\n",
    "# Mapping of A_TOD to 1, 2 and 3 \n",
    "time_of_day_mapping = {\n",
    "    1: 'Daytime',\n",
    "    2: 'Nighttime',\n",
    "    3: 'Unknown'\n",
    "}\n",
    "\n",
    "# create a new Time_of_Day column and mapping to it \n",
    "merged_all['Time_of_Day'] = merged_all['A_TOD'].map(time_of_day_mapping)\n",
    "\n",
    "# Create a count plot for the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=merged_all, x='Time_of_Day', palette=\"viridis\")\n",
    "plt.title('Distribution of Accidents by Time of Day (A_TOD)')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Day of week \n",
    "# Mapping of A_DOW\n",
    "day_of_week_mapping = {\n",
    "    1: 'Weekday',\n",
    "    2: 'Weekend',\n",
    "    3: 'Unknown'\n",
    "}\n",
    "\n",
    "# to create a new Day_of_Week column and mapping to it \n",
    "merged_all['Day_of_Week'] = merged_all['A_DOW'].map(day_of_week_mapping)\n",
    "\n",
    "# Create plot for distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=merged_all, x='Day_of_Week', palette=\"viridis\")\n",
    "plt.title('Distribution of Accidents by Day of Week (A_DOW)')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# age group columns and their labels\n",
    "age_group_columns = {\n",
    "    'A_D15_19': '15-19 yrs',\n",
    "    'A_D16_19': '16-19 yrs',\n",
    "    'A_D15_20': '15-20 yrs',\n",
    "    'A_D16_20': '16-20 yrs',\n",
    "    'A_D16_24': '16-24 yrs',\n",
    "    'A_D21_24': '21-24 yrs',\n",
    "    'A_D65PLS': '65+ yrs'\n",
    "}\n",
    "\n",
    "# Create a DataFrame \n",
    "age_group_data = pd.DataFrame()\n",
    "\n",
    "for col, label in age_group_columns.items():\n",
    "    temp_df = merged_all[merged_all[col] == 1][col].value_counts().rename_axis('Involvement').reset_index(name='Count')\n",
    "    temp_df['Age Group'] = label\n",
    "    age_group_data = pd.concat([age_group_data, temp_df], ignore_index=True)\n",
    "\n",
    "# a combined chart\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=age_group_data, x='Age Group', y='Count', hue='Involvement', palette=\"viridis\")\n",
    "plt.title('Distribution of Crashes Involving Different Age Groups')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Number of Crashes')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Mapping of A_BODY \n",
    "vehicle_body_mapping = {\n",
    "    1: 'Passenger Car',\n",
    "    2: 'Light Truck - Pickup',\n",
    "    3: 'Light Truck - Utility',\n",
    "    4: 'Light Truck - Van',\n",
    "    5: 'Light Truck - Other',\n",
    "    6: 'Large Truck',\n",
    "    7: 'Motorcycle',\n",
    "    8: 'Bus',\n",
    "    9: 'Other'\n",
    "}\n",
    "\n",
    "# create a new Vehicle_Type column\n",
    "merged_all['Vehicle_Type'] = merged_all['A_BODY'].map(vehicle_body_mapping)\n",
    "\n",
    "# Create a count plot for vehicle type distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=merged_all, x='Vehicle_Type', palette=\"viridis\")\n",
    "plt.title('Distribution of Vehicle Types in Accidents')\n",
    "plt.xlabel('Vehicle Type')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# new features \n",
    "\n",
    "print(merged_all[['FATALS', 'A_CRAINJ', 'A_POSBAC', 'A_DIST']].describe())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a histogram for FATALS column\n",
    "plt.figure(figsize=(10, 6))\n",
    "# The maximum number of fatalities \n",
    "max_fatalities = merged_all['FATALS'].max()\n",
    "sns.histplot(data=merged_all, x='FATALS', bins=range(1, max_fatalities + 1), kde=False)\n",
    "plt.title('Histogram of Number of Fatalities (FATALS)')\n",
    "plt.xlabel('Number of Fatalities')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(1, max_fatalities)  # Limiting x-axis to the max number of fatalities\n",
    "plt.xticks(range(1, max_fatalities + 1))  # Set x-axis ticks for each number of fatalities\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Mapping of A_POSBAC \n",
    "bac_test_result_mapping = {\n",
    "    1: 'Positive BAC',\n",
    "    2: 'Zero BAC',\n",
    "    3: 'Unknown BAC'\n",
    "}\n",
    "\n",
    "# create a new BAC_Test_Result column\n",
    "merged_all['BAC_Test_Result'] = merged_all['A_POSBAC'].map(bac_test_result_mapping)\n",
    "\n",
    "# Create a count plot for distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=merged_all, x='BAC_Test_Result', palette=\"viridis\")\n",
    "plt.title('Distribution of Accidents by BAC Test Results (A_POSBAC)')\n",
    "plt.xlabel('BAC Test Result')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Mapping of A_DIST codes\n",
    "distraction_involvement_mapping = {\n",
    "    1: 'Distracted Driver Involved',\n",
    "    2: 'Other Crash'\n",
    "}\n",
    "\n",
    "# to create a new Distracted_Driving column\n",
    "merged_all['Distracted_Driving'] = merged_all['A_DIST'].map(distraction_involvement_mapping)\n",
    "\n",
    "# Create a count plot for distracted driving distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=merged_all, x='Distracted_Driving', palette=\"viridis\")\n",
    "plt.title('Distribution of Accidents Involving Distracted Driving (A_DIST)')\n",
    "plt.xlabel('Distracted Driving Involvement')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Feature Engineering and Data Preprocessing\n\nThis section creates meaningful features from the categorical accident data to improve model performance."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1 Data Preprocessing Steps\n\n**Data Integration Strategy**:\n- Merged three FARS datasets (ACC_AUX, VEH_AUX, PER_AUX) using case numbers\n- Handled missing values by removing columns with significant missing data (>30%)\n- Ensured data consistency across merged datasets\n\n### 4.2 Feature Engineering Approach\n\n**New Feature Creation**:\n1. **Combined Risk Factors**: Created composite features combining alcohol involvement and distracted driving\n2. **Categorical Mappings**: Transformed coded variables into interpretable categories\n3. **Temporal Features**: Enhanced time-based variables for better pattern recognition\n\n**Rationale for Feature Engineering**:\n- **Alcohol and Distraction Combination**: Research shows these factors often interact to increase accident severity\n- **Age Group Analysis**: Different age demographics show distinct risk patterns requiring separate analysis\n- **Categorical Transformation**: Converting numeric codes to meaningful labels improves model interpretability\n\n**Data Transformation**:\n- **Categorical to Numeric**: Used one-hot encoding for categorical variables in machine learning models\n- **Standardization**: Applied to numerical features for clustering analysis to ensure equal weighting"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "# python code goes here, you can add more code cells below this one if needed\n",
    "# to better structure your JN\n",
    "# include any code you use to do data preparation and feature engineering so the process\n",
    "# can be replicated\n",
    "\n",
    "\n",
    "# Creating a new combined feature for alcohol and distraction involvement\n",
    "merged_all['Alcohol_and_Distraction'] = merged_all.apply(lambda x: 1 if x['A_POSBAC'] == 1 and x['A_DIST'] == 1 else 0, axis=1)\n",
    "\n",
    "merged_all.head()\n",
    "\n",
    "# Count the occurrences of each value in the 'Alcohol_and_Distraction' column\n",
    "feature_counts = merged_all['Alcohol_and_Distraction'].value_counts()\n",
    "\n",
    "# Create a bar chart to visualize the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "feature_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Alcohol and Distraction Involvement')\n",
    "plt.xlabel('Alcohol and Distraction Involvement')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Not Involved', 'Involved'], rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#contributing factors\n",
    "contributing_factors = ['A_POSBAC', 'A_DIST', 'A_RD', 'A_ROLL']  \n",
    "\n",
    "# Labels for age groups in A_AGE3\n",
    "age_group_3_labels = {\n",
    "    1: \"0-3\", 2: \"4-7\", 3: \"8-12\", 4: \"13-15\", 5: \"16-20\", 6: \"21-24\", 7: \"25-34\", \n",
    "    8: \"35-44\", 9: \"45-54\", 10: \"55-64\", 11: \"65-74\", 12: \"75+\", 13: \"Unknown\"\n",
    "}\n",
    "\n",
    "# Aggregating total fatalities and contributing factors for each age groupe \n",
    "total_fatalities_age3 = merged_all.groupby('A_AGE3')['FATALS'].sum()\n",
    "factors_age3 = merged_all.groupby('A_AGE3')[contributing_factors].sum()\n",
    "\n",
    "# Adding labels to the data to make it more clear \n",
    "total_fatalities_age3.index = total_fatalities_age3.index.map(age_group_3_labels)\n",
    "factors_age3.index = factors_age3.index.map(age_group_3_labels)\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "total_fatalities_age3_df = total_fatalities_age3.reset_index()\n",
    "total_fatalities_age3_df.columns = ['Age Group', 'Total Fatalities']\n",
    "factors_age3_df = factors_age3.reset_index()\n",
    "\n",
    "# Plotting total fatalities\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Age Group', y='Total Fatalities', data=total_fatalities_age3_df, palette=\"viridis\")\n",
    "plt.title('Total Fatalities by Age Group (A_AGE3)')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Total Fatalities')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "merged_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Supervised Learning: Fatality Prediction Model\n\n**Objective**: Predict the number of fatalities in traffic accidents using regional and temporal factors.\n\n**Feature Selection Rationale**:\n- **A_REGION**: Geographic regions show different traffic patterns, infrastructure quality, and emergency response capabilities\n- **A_DOW**: Day of week captures different driving behaviors (commuter vs. recreational traffic)\n\n**Model Choice**: Linear Regression\n- Simple and interpretable model suitable for continuous target variable (fatality count)\n- Baseline model to establish fundamental relationships before exploring complex algorithms\n\n**Evaluation Method**: 10-fold Cross-Validation with Mean Squared Error\n- Provides robust performance estimation across different data subsets\n- MSE is appropriate for regression tasks with count data"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Supervised Learning: Linear Regression for Fatality Prediction\n\n# Select predictor variables based on domain knowledge\nX = merged_all[['A_REGION', 'A_DOW']]\n\n# Define target variable (number of fatalities)\ny = merged_all['FATALS']\n\nprint(f\"Features: {X.columns.tolist()}\")\nprint(f\"Target: FATALS (Number of fatalities)\")\nprint(f\"Dataset size: {X.shape[0]} samples, {X.shape[1]} features\")\n\n# One-hot encoding for categorical variables\ncolumn_transformer = ColumnTransformer([\n    (\"region_encoder\", OneHotEncoder(), [\"A_REGION\"]),\n    (\"dow_encoder\", OneHotEncoder(), [\"A_DOW\"])\n], remainder='passthrough')\n\nX_transformed = column_transformer.fit_transform(X)\nprint(f\"Features after encoding: {X_transformed.shape[1]} dimensions\")\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n\n# Initialize and train Linear Regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# 10-fold Cross-validation\ncv_scores = cross_val_score(model, X_transformed, y, cv=10, scoring='neg_mean_squared_error')\n\n# Convert to positive MSE scores\nmse_scores = -cv_scores\ncv_mean = mse_scores.mean()\ncv_std = mse_scores.std()\n\nprint(f'\\n10-Fold Cross-Validation Results:')\nprint(f'Mean MSE: {cv_mean:.4f}')\nprint(f'Standard Deviation: {cv_std:.4f}')\nprint(f'MSE Range: [{cv_mean - cv_std:.4f}, {cv_mean + cv_std:.4f}]')\n\n# Visualize cross-validation results\nplt.figure(figsize=(10, 6))\nplt.errorbar(np.arange(1, 11), mse_scores, yerr=cv_std, fmt='o', capsize=5, capthick=2)\nplt.title('10-Fold Cross-Validation MSE Scores for Linear Regression')\nplt.xlabel('Fold Number')\nplt.ylabel('Mean Squared Error')\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Unsupervised Learning: K-Means Clustering Analysis\n\n**Objective**: Discover hidden patterns and group similar accidents to understand accident characteristics.\n\n**Features Selected**: \n- **A_REGION**: Geographic clustering reveals regional accident patterns\n- **FATALS**: Accident severity clustering identifies high-risk scenarios\n\n**Algorithm Choice**: K-Means Clustering\n- Well-suited for numerical data with clear geometric interpretation\n- Computationally efficient for large datasets\n- Provides interpretable centroids representing cluster characteristics\n\n**Distance Metric**: Euclidean Distance\n- Standard choice for K-means algorithm\n- Appropriate for numerical features after standardization\n- Captures both regional and severity similarities effectively\n\n**Optimization Strategy**: Elbow Method\n- Systematic approach to determine optimal number of clusters\n- Balances model complexity with clustering quality"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Unsupervised Learning: K-Means Clustering Analysis\n\n# Select features for clustering analysis\ncluster_data = merged_all[['A_REGION', 'FATALS']]\nprint(f\"Clustering features: {cluster_data.columns.tolist()}\")\nprint(f\"Data shape: {cluster_data.shape}\")\n\n# Standardize features for clustering\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(cluster_data)\nprint(\"Features standardized for equal weighting in clustering\")\n\n# Elbow Method to determine optimal number of clusters\nwcss = []  # Within-Cluster Sum of Squares\nk_range = range(1, 11)\n\nprint(\"Computing WCSS for different cluster numbers...\")\nfor i in k_range:\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(scaled_data)\n    wcss.append(kmeans.inertia_)\n\n# Plot Elbow Method results\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, wcss, 'bo-', linewidth=2, markersize=8)\nplt.title('Elbow Method for Optimal Number of Clusters', fontsize=14)\nplt.xlabel('Number of Clusters (k)', fontsize=12)\nplt.ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.xticks(k_range)\n\n# Highlight the elbow point (k=3 based on visual inspection)\nplt.axvline(x=3, color='red', linestyle='--', alpha=0.7, label='Optimal k=3')\nplt.legend()\nplt.show()\n\n# Apply K-Means with optimal number of clusters\noptimal_clusters = 3\nprint(f\"\\nApplying K-Means with k={optimal_clusters} clusters\")\n\nkmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)\ncluster_labels = kmeans.fit_predict(scaled_data)\n\n# Add cluster labels to original dataset\nmerged_all['Cluster_Labels'] = cluster_labels\n\n# Display cluster centroids (in original scale)\ncentroids_scaled = kmeans.cluster_centers_\ncentroids_original = scaler.inverse_transform(centroids_scaled)\n\nprint(f\"\\nCluster Centroids (Original Scale):\")\nprint(\"Cluster | A_REGION | FATALS\")\nprint(\"-\" * 30)\nfor i, centroid in enumerate(centroids_original):\n    print(f\"   {i}    |   {centroid[0]:.2f}   |  {centroid[1]:.2f}\")\n\n# Cluster distribution\ncluster_counts = merged_all['Cluster_Labels'].value_counts().sort_index()\nprint(f\"\\nCluster Distribution:\")\nfor i in range(optimal_clusters):\n    print(f\"Cluster {i}: {cluster_counts[i]} accidents ({cluster_counts[i]/len(merged_all)*100:.1f}%)\")\n\n# Visualize clusters\nplt.figure(figsize=(10, 6))\ncolors = ['red', 'blue', 'green', 'purple', 'orange']\nfor i in range(optimal_clusters):\n    cluster_data_subset = merged_all[merged_all['Cluster_Labels'] == i]\n    plt.scatter(cluster_data_subset['A_REGION'], cluster_data_subset['FATALS'], \n                c=colors[i], label=f'Cluster {i}', alpha=0.6)\n\n# Plot centroids\nfor i, centroid in enumerate(centroids_original):\n    plt.scatter(centroid[0], centroid[1], c='black', marker='x', s=200, linewidths=3)\n    plt.annotate(f'C{i}', (centroid[0], centroid[1]), xytext=(5, 5), \n                textcoords='offset points', fontweight='bold')\n\nplt.title('K-Means Clustering Results: Regional and Fatality Patterns')\nplt.xlabel('A_REGION (Geographic Region)')\nplt.ylabel('FATALS (Number of Fatalities)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Conclusions and Key Findings\n\n### What I Learned from This Project\n\n**Data Science Skills**:\n- **Data Integration**: Successfully merged multiple large datasets with complex relationships\n- **Feature Engineering**: Created meaningful variables from categorical accident data\n- **Machine Learning**: Applied both supervised and unsupervised learning techniques effectively\n\n**Domain Knowledge**:\n- **Traffic Safety Patterns**: Discovered regional and temporal patterns in fatal accidents\n- **Risk Factor Analysis**: Identified key contributing factors to accident severity\n- **Policy Implications**: Generated insights relevant for traffic safety interventions\n\n### Project Limitations\n\n**Data Scope**:\n- **Single Year Analysis**: Limited to 2021 data - temporal trends across years not captured\n- **Fatal Accidents Only**: FARS data excludes non-fatal accidents, limiting generalizability\n- **Feature Selection**: Limited feature set for modeling - additional variables could improve predictions\n\n**Methodological Constraints**:\n- **Simple Models**: Used baseline algorithms - more complex models might capture non-linear relationships\n- **Regional Granularity**: State-level analysis may miss local traffic patterns and infrastructure differences\n\n### Technical Challenges Faced\n\n1. **Data Integration Complexity**: Merging three datasets with different granularities and relationships\n2. **Missing Data Handling**: Systematic approach needed for columns with significant missing values\n3. **Categorical Variable Encoding**: Proper transformation of FARS coded variables for machine learning\n4. **Scale Differences**: Standardization required for clustering due to different feature ranges\n\n### Algorithms and Model Selection\n\n**Linear Regression**:\n- **Why Chosen**: Interpretable baseline model for continuous target variable (fatality count)\n- **Advantages**: Simple interpretation, fast computation, established statistical properties\n- **Use Case**: Suitable for initial exploration of relationships between regional/temporal factors and fatalities\n\n**K-Means Clustering**:\n- **Why Chosen**: Efficient algorithm for identifying patterns in numerical data\n- **Advantages**: Clear geometric interpretation, computationally scalable, provides interpretable centroids\n- **Use Case**: Effective for discovering regional accident severity patterns\n\n### Future Work Opportunities\n\n**Enhanced Modeling**:\n- **Advanced Algorithms**: Implement Random Forest, XGBoost, or neural networks for better prediction accuracy\n- **Time Series Analysis**: Incorporate multi-year data to identify seasonal and yearly trends\n- **Ensemble Methods**: Combine multiple models for improved robustness and accuracy\n\n**Extended Feature Engineering**:\n- **Weather Integration**: Include detailed weather data for environmental factor analysis\n- **Road Infrastructure**: Add highway type, construction zones, and traffic signal data\n- **Socioeconomic Factors**: Incorporate demographic and economic variables by region\n\n**Real-World Applications**:\n- **Predictive Dashboard**: Deploy model as real-time accident risk assessment tool\n- **Policy Analysis**: Use insights for evidence-based traffic safety policy recommendations\n- **Resource Allocation**: Guide emergency response resource placement based on predicted high-risk areas"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. References and Data Sources\n\n### Primary Data Source\n**National Highway Traffic Safety Administration (NHTSA)**. (2021). *Fatality Analysis Reporting System (FARS)*. \n- **Website**: https://www.nhtsa.gov/research-data/fatality-analysis-reporting-system-fars\n- **Description**: Comprehensive nationwide census of fatal motor vehicle traffic crashes\n- **Data Files Used**: ACC_AUX.CSV, VEH_AUX.CSV, PER_AUX.CSV (2021 dataset)\n\n### Technical Documentation\n**Python Libraries and Frameworks**:\n- **Pandas**: McKinney, W. (2010). Data structures for statistical computing in Python. *Proceedings of the 9th Python in Science Conference*.\n- **Scikit-learn**: Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12, 2825-2830.\n- **Matplotlib**: Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. *Computing in Science & Engineering*, 9(3), 90-95.\n\n### Methodological References\n**Machine Learning Techniques**:\n- **K-Means Clustering**: MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. *Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability*.\n- **Cross-Validation**: Stone, M. (1974). Cross-validatory choice and assessment of statistical predictions. *Journal of the Royal Statistical Society*, 36(2), 111-147.\n\n### Traffic Safety Research Context\n**FARS Data Applications**:\n- **NHTSA Traffic Safety Facts**: Annual reports utilizing FARS data for policy development and safety analysis\n- **Academic Research**: Numerous peer-reviewed studies using FARS data for transportation safety research\n\n*This project demonstrates academic application of data science techniques to real-world transportation safety data for educational and portfolio purposes.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}